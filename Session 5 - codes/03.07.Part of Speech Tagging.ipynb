{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "string = \"I was watching movies\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('was', 'VBD'), ('watching', 'VBG'), ('movies', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "print(pos_tag(word_tokenize(string)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PRP: Personal pronoun\n",
    "# VBD: Verb, past tense\n",
    "# VBG: Veb, gerund\n",
    "# NNS: Noun plural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Retrieving all nouns\n",
    "s = 'My favourite scientist is Carl Sagan'\n",
    "tagged = pos_tag(word_tokenize(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scientist', 'Carl', 'Sagan']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allnoun = [word for word, pos in tagged if pos in ['NN','NNP']]\n",
    "allnoun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Brown Corpus was the first million-word electronic corpus of English, created in 1961 at Brown University. This corpus contains text from 500 sources, and the sources have been categorized by genre, such as news, editorial, and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tags = [tag for (word, tag) in brown.tagged_words(categories='news')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('VBD', 2524),\n",
       " ('CC', 2664),\n",
       " ('JJ', 4392),\n",
       " ('.', 4452),\n",
       " ('NNS', 5066),\n",
       " (',', 5133),\n",
       " ('NP', 6866),\n",
       " ('AT', 8893),\n",
       " ('IN', 10616),\n",
       " ('NN', 13162)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "freq = nltk.FreqDist(tags)\n",
    "tags_freq = sorted(freq.items(), key=operator.itemgetter(1))\n",
    "tags_freq[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Tagger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13089484257215028\n"
     ]
    }
   ],
   "source": [
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "print(default_tagger.accuracy(brown_tagged_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram Tagger:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram tagger takes previous n words in the context, to predict the POS tag for the given token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import DefaultTagger\n",
    "from nltk.tag import BigramTagger\n",
    "from nltk.tag import TrigramTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# splitting the data into train and test datasets\n",
    "train_data = brown_tagged_sents[:int(len(brown_tagged_sents) * 0.9)]\n",
    "test_data = brown_tagged_sents[int(len(brown_tagged_sents) * 0.9):] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8361407355726104\n"
     ]
    }
   ],
   "source": [
    "#unigram considers the conditional frequency of tags and predicts the most\n",
    "#frequent tag for the every given token.\n",
    "unigram_tagger = UnigramTagger(train_data, backoff=default_tagger)\n",
    "print(unigram_tagger.accuracy(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8452108043456593\n"
     ]
    }
   ],
   "source": [
    "#bigram consider the tags of the given word and previous word, and tag as\n",
    "#tuple to get the given tag for the test word.\n",
    "bigram_tagger = BigramTagger(train_data, backoff=unigram_tagger)\n",
    "print(bigram_tagger.accuracy(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.843317053722715\n"
     ]
    }
   ],
   "source": [
    "#trigram looks for the previous two words with the similar process.\n",
    "trigram_tagger = TrigramTagger(train_data, backoff=bigram_tagger)\n",
    "print(trigram_tagger.accuracy(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#We are combining the three taggers. First it will look for the Trigram\n",
    "#of the given word sequence for predicting the tag, if not found it Backoff \n",
    "#to BigramTagger parameter and to a UnigramTagger and in the end to a NN tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tag.sequential import RegexpTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "regexp_tagger = RegexpTagger(\n",
    "[(r'^-?[0-9]+(.[0-9]+)?$', 'CD'), #cardinal numbers\n",
    " (r'(The|the|A|a|An|an)$', 'AT'), #articles\n",
    " (r'.*able$', 'JJ'), #adjectives\n",
    " (r'.*ness$', 'NN'), #nouns formed from adj\n",
    " (r'.*ly$', 'RB'), #adverbs\n",
    " (r'.*s$', 'NNS'), #plural nouns\n",
    " (r'.*ing$', 'VRG'), #gerunds\n",
    " (r'.*ed$', 'VBD'), #past tense verbs\n",
    " (r'.*', 'NN') # nouns (default)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2999102960231237\n"
     ]
    }
   ],
   "source": [
    "print(regexp_tagger.accuracy(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the ne_chunk method recognizes people(names), places(location),\n",
    "#and organizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"Stephen Hawking teach maths at the Oxford University in England\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Lena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Stephen/NNP)\n",
      "  Hawking/NNP\n",
      "  teach/VB\n",
      "  maths/NNS\n",
      "  at/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION Oxford/NNP University/NNP)\n",
      "  in/IN\n",
      "  (GPE England/NNP))\n"
     ]
    }
   ],
   "source": [
    "print(ne_chunk(nltk.pos_tag(word_tokenize(text)), binary=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NE Stephen/NNP)\n",
      "  Hawking/NNP\n",
      "  teach/VB\n",
      "  maths/NNS\n",
      "  at/IN\n",
      "  the/DT\n",
      "  (NE Oxford/NNP University/NNP)\n",
      "  in/IN\n",
      "  (NE England/NNP))\n"
     ]
    }
   ],
   "source": [
    "# if bynary parameter is True it provides the output for the entire\n",
    "# sentence tree and tags everything.\n",
    "print(ne_chunk(nltk.pos_tag(word_tokenize(text)), binary=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References: \n",
    "\n",
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "\n",
    "http://www.nltk.org/book/ch02.html\n",
    "\n",
    "https://nlp.stanford.edu/software/\n",
    "\n",
    "http://www.nltk.org/api/nltk.tag.html#module-nltk.tag.stanford\n",
    "\n",
    "https://en.wikipedia.org/wiki/Brown_Corpus\n",
    "\n",
    "https://nlp.stanford.edu/software/CRF-NER.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
