{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words:\n",
    "\n",
    "Stop words are the words in a stop list (or stoplist or negative dictionary) which are filtered out (i.e. stopped) before or after processing of natural language data (text) because they are insignificant. There is no single universal list of stop words used by all natural language processing tools, nor any agreed upon rules for identifying stop words, and indeed not all tools even use such a list. Therefore, any group of words can be chosen as the stop words for a given purpose. The \"general trend in [information retrieval] systems over time has been from standard use of quite large stop lists (200–300 terms) to very small stop lists (7–12 terms) to no stop list whatsoever\"\n",
    "\n",
    "(https://en.wikipedia.org/wiki/Stop_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#NLTK supports 22 languages for removing the stop words\n",
    "stopw = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = 'epic adventure in time, space and life.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['epic', 'adventure', 'time,', 'space', 'life.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanwords = [word for word in text.split() if word not in stopw]\n",
    "cleanwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text1 = 'this is just a programm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['programm']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanwords1 = [word for word in text1.split() if word not in stopw]\n",
    "cleanwords1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text, lang='english'):\n",
    "    words = word_tokenize(text)\n",
    "    stopw = stopwords.words(lang)\n",
    "    stopw_removed = [word for word in words if word.lower() not in stopw]\n",
    "    return \" \".join(stopw_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great course NLP Python NLTK\n"
     ]
    }
   ],
   "source": [
    "print(remove_stopwords('This a great course of NLP with Python and NLTK'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing rare words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = 'One of the basic rules of the universe is that nothing is perfect. Perfection simply doesnt exist, without imperfection, neither you nor I would exist. Sthepen Hawking.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one of the basic rules of the universe is that nothing is perfect. perfection simply doesnt exist, without imperfection, neither you nor i would exist. sthepen hawking.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg = msg.lower()\n",
    "msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'basic',\n",
       " 'rules',\n",
       " 'of',\n",
       " 'the',\n",
       " 'universe',\n",
       " 'is',\n",
       " 'that',\n",
       " 'nothing',\n",
       " 'is',\n",
       " 'perfect',\n",
       " '.',\n",
       " 'perfection',\n",
       " 'simply',\n",
       " 'doesnt',\n",
       " 'exist',\n",
       " ',',\n",
       " 'without',\n",
       " 'imperfection',\n",
       " ',',\n",
       " 'neither',\n",
       " 'you',\n",
       " 'nor',\n",
       " 'i',\n",
       " 'would',\n",
       " 'exist',\n",
       " '.',\n",
       " 'sthepen',\n",
       " 'hawking',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = word_tokenize(msg)\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "freqd = FreqDist(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('one', 1), ('of', 2), ('the', 2), ('basic', 1), ('rules', 1), ('universe', 1), ('is', 2), ('that', 1), ('nothing', 1), ('perfect', 1), ('.', 3), ('perfection', 1), ('simply', 1), ('doesnt', 1), ('exist', 2), (',', 2), ('without', 1), ('imperfection', 1), ('neither', 1), ('you', 1), ('nor', 1), ('i', 1), ('would', 1), ('sthepen', 1), ('hawking', 1)])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqd.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('one', 1),\n",
       " ('basic', 1),\n",
       " ('rules', 1),\n",
       " ('universe', 1),\n",
       " ('that', 1),\n",
       " ('nothing', 1),\n",
       " ('perfect', 1),\n",
       " ('perfection', 1),\n",
       " ('simply', 1),\n",
       " ('doesnt', 1),\n",
       " ('without', 1),\n",
       " ('imperfection', 1),\n",
       " ('neither', 1),\n",
       " ('you', 1),\n",
       " ('nor', 1),\n",
       " ('i', 1),\n",
       " ('would', 1),\n",
       " ('sthepen', 1),\n",
       " ('hawking', 1),\n",
       " ('of', 2),\n",
       " ('the', 2),\n",
       " ('is', 2),\n",
       " ('exist', 2),\n",
       " (',', 2),\n",
       " ('.', 3)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "sorted_freq = sorted(freqd.items(), key=operator.itemgetter(1))\n",
    "sorted_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('one', 1),\n",
       " ('basic', 1),\n",
       " ('rules', 1),\n",
       " ('universe', 1),\n",
       " ('that', 1),\n",
       " ('nothing', 1),\n",
       " ('perfect', 1),\n",
       " ('perfection', 1),\n",
       " ('simply', 1),\n",
       " ('doesnt', 1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rarewords = sorted_freq[:10]\n",
    "rarewords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'basic', 'rules', 'universe', 'that', 'nothing', 'perfect', 'perfection', 'simply', 'doesnt']\n"
     ]
    }
   ],
   "source": [
    "rare = []\n",
    "for i in rarewords:\n",
    "    rare.append(i[0])\n",
    "print (rare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['of',\n",
       " 'the',\n",
       " 'of',\n",
       " 'the',\n",
       " 'is',\n",
       " 'is',\n",
       " '.',\n",
       " 'exist',\n",
       " ',',\n",
       " 'without',\n",
       " 'imperfection',\n",
       " ',',\n",
       " 'neither',\n",
       " 'you',\n",
       " 'nor',\n",
       " 'i',\n",
       " 'would',\n",
       " 'exist',\n",
       " '.',\n",
       " 'sthepen',\n",
       " 'hawking',\n",
       " '.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalwords = [w for w in token if w not in rare]\n",
    "finalwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spell correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.metrics import edit_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate the Levenshtein edit-distance between two strings.\n",
    "#The edit distance is the number of characters that need to be\n",
    "# substituted, inserted, or deleted, to transform s1 into s2\n",
    "edit_distance(\"rain\", \"shine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transforming \"rain\" to \"shine\" requires three steps,\n",
    "# consisting of two substitutions and one insertion:\n",
    "# \"rain\" -> \"sain\" -> \"shin\" -> \"shine\".  These operations could have\n",
    "# been done in other orders, but at least three steps are needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference:\n",
    "\n",
    "http://norvig.com/spell-correct.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
